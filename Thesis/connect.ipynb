{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c01082b6",
   "metadata": {},
   "source": [
    "# Connect companies universe with PE_backed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ea3040bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_companies = pd.read_csv(r\"C:\\Users\\minha\\OneDrive\\Courses\\Thesis\\final _datasets\\companies_universe_repiq.csv\")\n",
    "df_PE_backed = pd.read_csv(r\"C:\\Users\\minha\\OneDrive\\Courses\\Thesis\\final _datasets\\PE_backed.csv\")\n",
    "\n",
    "# Replace NaN with empty lists in the 'PE_Firm' column\n",
    "df_PE_backed['PE_Firm'] = df_PE_backed['PE_Firm'].apply(lambda x: [] if pd.isna(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9c33d89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PE_id_list = list(df_PE_backed[\"Target ID\"].unique())\n",
    "companies_uni_id = list(df_companies[\"SP_ENTITY_ID\"].unique())\n",
    "\n",
    "# First, convert PE_id_list to a set for efficiency\n",
    "PE_id_set = set(PE_id_list)\n",
    "\n",
    "# Filter df_companies to keep only rows where SP_ENTITY_ID is NOT in PE_id_list\n",
    "df_companies_cleaned = df_companies[~df_companies[\"SP_ENTITY_ID\"].isin(PE_id_set)].copy()\n",
    "df_companies_cleaned.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ee4a3dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Years you want\n",
    "years = list(range(2007, 2021))\n",
    "\n",
    "# Create a list to hold new rows\n",
    "new_rows = []\n",
    "\n",
    "# Iterate over cleaned companies\n",
    "for idx, row in df_companies_cleaned.iterrows():\n",
    "    for year in years:\n",
    "        new_rows.append({\n",
    "            \"Company\": row[\"SP_ENTITY_NAME\"],  \n",
    "            \"Target ID\": row[\"SP_ENTITY_ID\"],\n",
    "            \"Target\": row[\"SP_ENTITY_NAME\"],\n",
    "            \"Year\": year,\n",
    "            \"PE_Firm\": [],\n",
    "            \"PE-backed\": 0,\n",
    "            \"reprisk_id\": row[\"reprisk_id\"]  \n",
    "        })\n",
    "\n",
    "# Create a DataFrame from the new rows\n",
    "df_companies_expanded = pd.DataFrame(new_rows)\n",
    "\n",
    "# Combine with PE-backed dataset\n",
    "df_id_repiq = pd.concat([df_PE_backed, df_companies_expanded], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8a273b",
   "metadata": {},
   "source": [
    "# Calculation for yearly Reprisk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a39080fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reprisk = pd.read_csv(r\"C:\\Users\\minha\\OneDrive\\Courses\\Thesis\\RepRisk Index.csv\")\n",
    "\n",
    "columns_to_drop = [\n",
    "    'name', \n",
    "    'headquarter_country', \n",
    "    'url', \n",
    "    'all_ISINs', \n",
    "    'primary_ISIN', \n",
    "    'No_reported_risk_exposure'\n",
    "]\n",
    "\n",
    "# Drop the columns\n",
    "df_reprisk = df_reprisk.drop(columns=columns_to_drop)\n",
    "\n",
    "# Ensure your 'date' column is in datetime format\n",
    "df_reprisk['date'] = pd.to_datetime(df_reprisk['date'])\n",
    "df_reprisk['peak_RRI_date'] = pd.to_datetime(df_reprisk['peak_RRI_date'], errors='coerce')\n",
    "\n",
    "# Create a 'year' column\n",
    "df_reprisk['year'] = df_reprisk['date'].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41090ed6",
   "metadata": {},
   "source": [
    "## yearly calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9f47592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode(series):\n",
    "    if series.dropna().empty:\n",
    "        return None\n",
    "    return series.mode().iloc[0] if not series.mode().empty else None\n",
    "\n",
    "# Now group by RepRisk_ID and year\n",
    "df_reprisk_yearly = df_reprisk.groupby(['RepRisk_ID', 'year']).agg({\n",
    "    'current_RRI': 'mean',        # Take the maximum risk index in the year\n",
    "    'RRI_trend': 'sum',          # Average trend over the year\n",
    "    'peak_RRI': 'max',            # Max peak_RRI\n",
    "    'peak_RRI_date': 'max',     #  Latest date with peak RRI\n",
    "    'RepRisk_rating': mode,     \n",
    "    'country_sector_average': 'mean',  # Average across year\n",
    "    'environmental_percentage': 'last', # Take last known percentage\n",
    "    'social_percentage': 'last',\n",
    "    'governance_percentage': 'last',\n",
    "    'headquarter_country_code': 'first',  # Same across the year\n",
    "    'sectors': 'first'                   # Assuming sector doesn't change\n",
    "}).reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a460b105",
   "metadata": {},
   "source": [
    "## yearly snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a17aa7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by RepRisk_ID, year, date (latest date on top)\n",
    "df_reprisk = df_reprisk.sort_values(['RepRisk_ID', 'year', 'date'], ascending=[True, True, False])\n",
    "\n",
    "# Pick the first (latest) row in each (RepRisk_ID, year) group\n",
    "df_reprisk_yearly_snapshot = (\n",
    "    df_reprisk\n",
    "    .groupby(['RepRisk_ID', 'year'], as_index=False)\n",
    "    .first()\n",
    ")\n",
    "\n",
    "# Drop 'date' column if you don't want it\n",
    "df_reprisk_yearly_snapshot = df_reprisk_yearly_snapshot.drop(columns=['date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "be85b5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the merge\n",
    "merged_index_snapshot_df = pd.merge(\n",
    "    df_id_repiq,\n",
    "    df_reprisk_yearly_snapshot,\n",
    "    left_on=['Year', 'reprisk_id'],\n",
    "    right_on=['year', 'RepRisk_ID'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Optional: if you want to drop the duplicate 'year' and 'RepRisk_ID' columns after the merge\n",
    "merged_index_snapshot_df = merged_index_snapshot_df.drop(columns=['year', 'RepRisk_ID'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28775e4b",
   "metadata": {},
   "source": [
    "# Connect incidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "89a4bf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incidents_w_id= pd.read_csv(r\"C:\\Users\\minha\\OneDrive\\Courses\\Thesis\\RepRisk Incidents.csv\")\n",
    "df_incidents = pd.read_csv(r\"demo/not_filtered_incidents.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b8a09ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# --- 1. Clean names (remove punctuation, normalize spaces) ---\n",
    "suffixes_pattern = r'\\b(llc|inc|ltd|corp|gmbh|spa|sas|plc|co|company|limited)\\b'\n",
    "\n",
    "def clean_name(name):\n",
    "    if isinstance(name, str):\n",
    "        name = name.lower()\n",
    "        name = re.sub(r'[^\\w\\s]', '', name)         # Remove punctuation\n",
    "        name = re.sub(suffixes_pattern, '', name)   # Remove suffixes\n",
    "        name = re.sub(r'\\s+', ' ', name)             # Normalize spaces\n",
    "        return name.strip()\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "689f496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_incidents['cleaned_company_name'] = df_incidents['company_name'].apply(clean_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8f72088d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from cleaned company name to the first reprisk_id found\n",
    "name_to_id_map = df_incidents_w_id.drop_duplicates('company_name').set_index('company_name')['reprisk_id']\n",
    "\n",
    "# Map the reprisk_id to df_incidents using the cleaned_company_name column\n",
    "df_incidents['reprisk_id'] = df_incidents['company_name'].map(name_to_id_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bfbe395a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, ensure the year columns are properly named/aligned for merging\n",
    "df_incidents = df_incidents.rename(columns={'year': 'Year'})  # rename 'year' in df_incidents to 'Year' if necessary\n",
    "\n",
    "# Now perform the merge\n",
    "df_reprisk_yearly_snapshot_w_incidents = pd.merge(\n",
    "    merged_index_snapshot_df,\n",
    "    df_incidents,\n",
    "    how='left', \n",
    "    on=['reprisk_id', 'Year'])\n",
    "\n",
    "df_reprisk_yearly_snapshot_w_incidents = df_reprisk_yearly_snapshot_w_incidents.drop(columns=['cleaned_company_name', 'company_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a34d5eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reprisk_yearly_snapshot_w_incidents.to_csv(\"final_dataset_NEWEST.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77e06f0",
   "metadata": {},
   "source": [
    "# Examine datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "45fa6ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1059\n",
      "4613\n"
     ]
    }
   ],
   "source": [
    "PE_unique = df_id_repiq[df_id_repiq[\"PE-backed\"] == 1][\"reprisk_id\"].nunique()\n",
    "no_PE_unique = df_id_repiq[df_id_repiq[\"PE-backed\"] == 0][\"reprisk_id\"].nunique()\n",
    "print(PE_unique)\n",
    "print(no_PE_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "62e77bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Company', 'Target ID', 'Target', 'Year', 'PE_Firm', 'PE-backed',\n",
       "       'reprisk_id', 'current_RRI', 'RRI_trend', 'peak_RRI', 'peak_RRI_date',\n",
       "       'RepRisk_rating', 'country_sector_average', 'environmental_percentage',\n",
       "       'social_percentage', 'governance_percentage',\n",
       "       'headquarter_country_code', 'sectors', 'Total Climate Incidents',\n",
       "       'Max Severity Score', 'GHG-Related Incidents',\n",
       "       'Pollution-Related Incidents', 'UNGC Principle 7 Violations',\n",
       "       'UNGC Principle 8 Violations', 'UNGC Principle 9 Violations'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reprisk_yearly_snapshot_w_incidents.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4067996d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1061"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_id = df_reprisk_yearly_snapshot_w_incidents[\"reprisk_id\"].nunique()\n",
    "unique_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "59cec5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     reprisk_id  Non-NA and Non-Zero Climate Incidents Count\n",
      "0            30                                           14\n",
      "1            76                                           13\n",
      "2            80                                           14\n",
      "3            85                                           10\n",
      "4            91                                           13\n",
      "..          ...                                          ...\n",
      "372     1507307                                            1\n",
      "373     1692694                                            1\n",
      "374     1709987                                            1\n",
      "375     1772214                                            1\n",
      "376     2022922                                            1\n",
      "\n",
      "[377 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Filter the DataFrame to keep only non-NA and non-zero values\n",
    "filtered_df = df_reprisk_yearly_snapshot_w_incidents[\n",
    "    df_reprisk_yearly_snapshot_w_incidents[\"Total Climate Incidents\"].notna()\n",
    "    & (df_reprisk_yearly_snapshot_w_incidents[\"Total Climate Incidents\"] != 0)\n",
    "]\n",
    "\n",
    "# Group by 'RepRisk_ID' and count valid entries\n",
    "result = (\n",
    "    filtered_df.groupby(\"reprisk_id\")[\"Total Climate Incidents\"].count().reset_index()\n",
    ")\n",
    "\n",
    "# Rename the count column for clarity (optional)\n",
    "result = result.rename(\n",
    "    columns={\"Total Climate Incidents\": \"Non-NA and Non-Zero Climate Incidents Count\"}\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5cd6f06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 OLS Regression Results                                 \n",
      "========================================================================================\n",
      "Dep. Variable:     Q(\"Total Climate Incidents\")   R-squared:                       0.006\n",
      "Model:                                      OLS   Adj. R-squared:                 -0.005\n",
      "Method:                           Least Squares   F-statistic:                    0.5481\n",
      "Date:                          Fri, 02 May 2025   Prob (F-statistic):              0.905\n",
      "Time:                                  12:03:37   Log-Likelihood:                -3831.7\n",
      "No. Observations:                          1229   AIC:                             7693.\n",
      "Df Residuals:                              1214   BIC:                             7770.\n",
      "Df Model:                                    14                                         \n",
      "Covariance Type:                      nonrobust                                         \n",
      "===================================================================================\n",
      "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------\n",
      "Intercept           2.8182      1.234      2.283      0.023       0.396       5.240\n",
      "C(Year)[T.2008]    -0.4987      1.400     -0.356      0.722      -3.245       2.248\n",
      "C(Year)[T.2009]    -0.5651      1.528     -0.370      0.711      -3.562       2.432\n",
      "C(Year)[T.2010]    -0.0588      1.452     -0.041      0.968      -2.907       2.789\n",
      "C(Year)[T.2011]     0.6780      1.382      0.490      0.624      -2.034       3.390\n",
      "C(Year)[T.2012]    -0.0095      1.332     -0.007      0.994      -2.622       2.603\n",
      "C(Year)[T.2013]     0.4619      1.337      0.345      0.730      -2.162       3.085\n",
      "C(Year)[T.2014]     0.7317      1.346      0.544      0.587      -1.909       3.372\n",
      "C(Year)[T.2015]     0.2573      1.312      0.196      0.844      -2.316       2.831\n",
      "C(Year)[T.2016]    -0.2802      1.345     -0.208      0.835      -2.920       2.359\n",
      "C(Year)[T.2017]     0.2243      1.348      0.166      0.868      -2.421       2.870\n",
      "C(Year)[T.2018]    -0.0260      1.359     -0.019      0.985      -2.692       2.640\n",
      "C(Year)[T.2019]     0.0903      1.346      0.067      0.947      -2.550       2.730\n",
      "C(Year)[T.2020]     0.0091      1.349      0.007      0.995      -2.637       2.656\n",
      "Q(\"PE-backed\")     -0.8408      0.514     -1.636      0.102      -1.849       0.167\n",
      "==============================================================================\n",
      "Omnibus:                     1673.937   Durbin-Watson:                   0.427\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           305821.641\n",
      "Skew:                           7.572   Prob(JB):                         0.00\n",
      "Kurtosis:                      78.781   Cond. No.                         31.0\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from patsy import dmatrices\n",
    "\n",
    "# Drop rows with missing values in key columns\n",
    "df = df_reprisk_yearly_snapshot_w_incidents.dropna(subset=['PE-backed', 'Total Climate Incidents'])\n",
    "\n",
    "# Regression with year dummies using Q() to quote variable names with spaces\n",
    "model = smf.ols('Q(\"Total Climate Incidents\") ~ Q(\"PE-backed\") + C(Year)', data=df).fit()\n",
    "\n",
    "# Print regression summary\n",
    "print(model.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speeches",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
